from collections import Counter
from itertools import product
from typing import List, Tuple, Dict

import nmslib
import numpy as np
import pandas as pd
from sqlalchemy.dialects.mssql.information_schema import columns

from classes.base_data import BaseData
from classes.base_landlord_data import AddressValidator, BaseLandlordData, CleanCorpLLC, StringMatching, NetworkAnalysis
from constants.constants import DATA_ROOT

import warnings

from constants.constants import LANDLORD_DTYPES

warnings.filterwarnings("ignore")

class BaseLandlordWorkflow(BaseData):

    """
    Base class for landlord data processing workflow. Contains class variables with file path names for required
    datasets (STATIC FILES) and datasets produced by the different stages of the workflow (WORKFLOW GENERATED FILES).
    Each subclass represents a different stage in the workflow, with pre-defined inputs and outputs that isolate the
    logic required for processing each stage within the subclass.

    Running this workflow requires the following static datasets:

    SCRAPED PROPERTY TAXPAYER DATA
    GEOCODIO-VALIDATED ADDRESSES
    MASTER NAME FILE (TO DETECT NAME PATTERNS IN TAXPAYER NAMES)
    COMMON NAMES TO EXCLUDE FROM MATCHING
    COMMON ADDRESSES TO EXCLUDE FROM MATCHING
    LANDLORD ORGANIZATION ADDRESSES
    CORPORATE & LLC DATA
    ANCILLARY PROPERTY DATASETS CONTAINING CLASS CODES (FOR RENTAL SUBSETTING)
    ANCILLARY MISCELLANEOUS DATASETS (MANUALLY ANALYZED & FIXED ADDRESSES, ETC.)

    Note on filenames: all files with the "wkfl_" prefix are files generated by the workflow. Files without this prefix
    are considered static.

    Note on subclass initializers: Each initializer loads dataframes into the instance using the CSV filepaths stored
    as class variables in this base class. If lists of common names & addresses are needed, these are also initialized
    into the initializer.

    """

    # --------------------
    # ----STATIC FILES----
    # --------------------

    # Addresses & address validation
    PROP_VALIDATED = f"{DATA_ROOT}/addresses/prop_validated_new.csv"
    PROP_UNVALIDATED = f"{DATA_ROOT}/addresses/prop_unvalidated_new.csv"

    CORP_LLC_VALIDATED = f"{DATA_ROOT}/addresses/corp_llc_validated.csv"
    CORP_LLC_UNVALIDATED = f"{DATA_ROOT}/addresses/corp_llc_unvalidated.csv"

    TO_FIX = f"{DATA_ROOT}/addresses/to_fix.csv"
    TO_FIX_UNIT = f"{DATA_ROOT}/addresses/to_fix_unit.csv"
    TO_FIX_MISSING = f"{DATA_ROOT}/addresses/to_fix_missing.csv"

    ADDRESS_ANALYSIS_MASTER = f"{DATA_ROOT}/address_analysis/address_analysis_master.csv"

    # Property data
    COOK_SCRAPED_SUCCESS = f"{DATA_ROOT}/landlord_data/cook_scraped_success.csv"
    PROPS_LESS_6 = f"{DATA_ROOT}/landlord_data/Assessor_-_Single_and_Multi-Family_Improvement_Characteristics_20240902_FIXED.csv"
    PROPS_GREATER_7 = f"{DATA_ROOT}/landlord_data/Assessor_-_Commercial_Valuation_Data_20240902_FIXED.csv"
    PINS_NO_CLASS_RESULTS = f"{DATA_ROOT}/landlord_data/pins_no_class_results.csv"
    PROPS_MTO = f"{DATA_ROOT}/property_workflow/wkfl_props_mto.csv"

    # Common names & addresses
    MASTER_NAMES = f"{DATA_ROOT}/common/master_names.csv"
    COMMON_NAMES = f"{DATA_ROOT}/common/common_names.csv"
    COMMON_ADDRESSES = f"{DATA_ROOT}/common/common_addresses.csv"
    LANDLORD_ORG_ADDRESSES = f"{DATA_ROOT}/common/landlord_org_addresses.csv"

    # Corporate & LLC data
    CORP_DATA = f"{DATA_ROOT}/corp_llc/corp_data.csv"
    LLC_DATA = f"{DATA_ROOT}/corp_llc/llc_data.csv"

    # --------------------------------
    # ----WORKFLOW GENERATED FILES----
    # --------------------------------

    # WkflValidateAddresses, WkflFixAddresses
    WKFL_VALIDATED_MASTER = f"{DATA_ROOT}/landlord_workflow/wkfl_validated_master.csv"
    WKFL_UNVALIDATED_MASTER = f"{DATA_ROOT}/landlord_workflow/wkfl_unvalidated_master.csv"

    # WkflPrepareProps
    WKFL_PROPS_VALIDATED_CLASSES = f"{DATA_ROOT}/landlord_workflow/wkfl_props_validated_classes.csv"

    # WkflSubsetRentals
    WKFL_PROPS_RENTALS = f"{DATA_ROOT}/landlord_workflow/wkfl_props_rentals.csv"

    # WkflCleanTaxRecords
    WKFL_PROPS_RENTALS_CLEANED = f"{DATA_ROOT}/landlord_workflow/wkfl_props_rentals_cleaned.csv"

    # WkflPrepareCorpLLC
    WKFL_CORP_PROPS = f"{DATA_ROOT}/landlord_workflow/wkfl_corp_props.csv"
    WKFL_LLC_PROPS = f"{DATA_ROOT}/landlord_workflow/wkfl_llc_props.csv"
    WKFL_PROPS_ENTITIES = f"{DATA_ROOT}/landlord_workflow/wkfl_props_entities.csv"

    # WkflValidateCorpsLlcs
    WKFL_LLC_PROPS_VALIDATED = f"{DATA_ROOT}/landlord_workflow/wkfl_llc_props_validated.csv"
    WKFL_CORP_PROPS_VALIDATED = f"{DATA_ROOT}/landlord_workflow/wkfl_corp_props_validated.csv"
    WKFL_PROPS_ENTITIES_VALIDATED = f"{DATA_ROOT}/landlord_workflow/wkfl_props_entities_validated.csv"

    # WkflTaxStringMatch
    WKFL_PROPS_STRING_MATCHED = f"{DATA_ROOT}/landlord_workflow/wkfl_props_string_matched.csv"
    WKFL_PROPS_STRING_MATCHED_TEST = f"{DATA_ROOT}/landlord_workflow/wkfl_props_string_matched_test.csv"
    STRING_MATCH_SUMMARY_STATS = f"{DATA_ROOT}/landlord_workflow/string_match_summary_stats.csv"

    # WkflNetworkAnalysis
    WKFL_PROPS_NETWORKED = f"{DATA_ROOT}/landlord_workflow/wkfl_props_networked.csv"
    WKFL_PROPS_NETWORKED_TEST = f"{DATA_ROOT}/landlord_workflow/wkfl_props_networked_test3.csv"
    NETWORK_SUMMARY_STATS = f"{DATA_ROOT}/landlord_workflow/network_summary_stats2.csv"


def __init__(self):
        super().__init__()


class WkflValidateAddresses(BaseLandlordWorkflow, AddressValidator):

    """
    Validates addresses that were NOT validated by geocodio results filter.

    INPUTS:
        - Validated taxpayer addresses (from properties)
        - Unvalidated taxpayer addresses (from properties)
        - Validated corp/llc addresses (from corp/llc data)
        - Unvalidated corp/llc addresses (from corp/llc data)

    OUTPUTS:
        - Master file for validated addresses
        - Master file for unvalidated addresses
    """

    def __init__(self):
        super().__init__()
        # CREATE CONSTANT CLASS FOR EACH OF THESE DATASETS
        self.df_validated_props_in = self.get_df(self.PROP_VALIDATED, LANDLORD_DTYPES)
        self.df_validated_corp_llc_in = self.get_df(self.CORP_LLC_VALIDATED, LANDLORD_DTYPES)
        self.df_unvalidated_props_in = self.get_df(self.PROP_UNVALIDATED, LANDLORD_DTYPES)
        self.df_unvalidated_corp_llc_in = self.get_df(self.CORP_LLC_UNVALIDATED, LANDLORD_DTYPES)
        self.df_outputs = {}

    def drop_duplicates(self):
        df_validated_props = self.df_validated_props_in.drop_duplicates(subset=["RAW_ADDRESS"])
        df_validated_corps_llcs = self.df_validated_corp_llc_in.drop_duplicates(subset=["RAW_ADDRESS"])
        return df_validated_props, df_validated_corps_llcs

    def combine_unvalidated_addrs(self):
        df_unvalidated = pd.concat([self.df_unvalidated_props_in, self.df_unvalidated_corp_llc_in], ignore_index=True)
        df_unvalidated["IS_POBOX"] = df_unvalidated["RAW_ADDRESS"].apply(lambda x: self.set_is_pobox(x))
        return df_unvalidated

    def remove_validated(self, df_unvalidated, df_validated_props, df_validated_corps_llcs):
        df_validated_all = pd.concat([df_validated_props, df_validated_corps_llcs], ignore_index=True)
        unique_validated_addrs = list(df_validated_all["RAW_ADDRESS"].dropna().unique())
        df_unvalidated_filtered = self.remove_subset(df_unvalidated, "RAW_ADDRESS", unique_validated_addrs)
        return df_unvalidated_filtered

    def separate_poboxes(self, df_unvalidated_filtered):
        df_poboxes = df_unvalidated_filtered[df_unvalidated_filtered["IS_POBOX"] == True]
        df_others = df_unvalidated_filtered[df_unvalidated_filtered["IS_POBOX"] == False]
        return df_poboxes, df_others

    def fix_poboxes(self, df_poboxes):
        df_poboxes_fixed = df_poboxes.copy()
        df_poboxes_fixed["RAW_ADDRESS_CLEANED"] = df_poboxes_fixed["RAW_ADDRESS"].apply(lambda x: self.fix_pobox(x))
        return df_poboxes_fixed

    def run_validators(self, df_poboxes, df_others):
        df_validated_poboxes = self.run_pobox_validator(df_poboxes)
        df_validated_others = self.run_string_match_validator(df_others)
        return df_validated_poboxes, df_validated_others

    def concatenate_validated(self, df_validated_poboxes, df_validated_props, df_validated_orgs, df_validated_others):
        df_validated_new = pd.concat([df_validated_poboxes, df_validated_others], ignore_index=True)
        df_validated_master = pd.concat([df_validated_props, df_validated_orgs, df_validated_new], ignore_index=True)
        return df_validated_new, df_validated_master

    def generate_unvalidated_master(self, df_validated_new):
        validated_raw_addrs = list(df_validated_new["RAW_ADDRESS"].unique())
        df_unvalidated_master = pd.concat([
            self.df_unvalidated_corp_llc_in[~self.df_unvalidated_corp_llc_in["RAW_ADDRESS"].isin(validated_raw_addrs)],
            self.df_unvalidated_props_in[~self.df_unvalidated_props_in["RAW_ADDRESS"].isin(validated_raw_addrs)],
        ], ignore_index=True)
        return df_unvalidated_master

    def generate_validated_master(self, df_validated_master):
        df_validated_master["GCD_SECONDARYNUMBER"] = df_validated_master["GCD_SECONDARYNUMBER"].apply(lambda x: self.fix_zip(x))
        df_validated_master["GCD_XCOORD"] = df_validated_master.apply(lambda row: self.fix_xycoord(row, "x"), axis=1)
        df_validated_master["GCD_YCOORD"] = df_validated_master.apply(lambda row: self.fix_xycoord(row, "y"), axis=1)
        df_validated_master.drop(columns=["GCD_X", "GCD_Y"], inplace=True)
        df_validated_master["TAXPAYER_ZIP"] = df_validated_master["TAXPAYER_ZIP"].astype(str)
        df_validated_master["TAXPAYER_ZIP"] = df_validated_master["TAXPAYER_ZIP"].apply(lambda x: self.fix_zip(x))
        df_validated_master.dropna(subset=["GCD_CITY"], inplace=True)
        return df_validated_master

    def workflow(self):

        # EXECUTE WORKFLOW
        df_validated_props, df_validated_corps_llcs = self.drop_duplicates()
        df_unvalidated = self.combine_unvalidated_addrs()
        df_unvalidated_filtered = self.remove_validated(df_unvalidated, df_validated_props, df_validated_corps_llcs)
        df_poboxes, df_others = self.separate_poboxes(df_unvalidated_filtered)
        df_poboxes_fixed = self.fix_poboxes(df_poboxes)
        df_validated_poboxes, df_validated_others = self.run_validators(df_poboxes_fixed, df_others)
        df_validated_new, df_validated_master = self.concatenate_validated(
            df_validated_poboxes, df_validated_props, df_validated_corps_llcs, df_validated_others
        )
        df_unvalidated_master_final = self.generate_unvalidated_master(df_validated_new)
        df_validated_master_final = self.generate_validated_master(df_validated_master)

        # SET OUTPUTS
        self.df_outputs = {
            "validated_master": df_validated_master_final,
            "unvalidated_master": df_unvalidated_master_final
        }

    def save_outputs(self):
        self.save_df(self.df_outputs["validated_master"], self.WKFL_VALIDATED_MASTER, DATA_ROOT)
        self.save_df(self.df_outputs["unvalidated_master"], self.WKFL_UNVALIDATED_MASTER, DATA_ROOT)


class WkflFixAddresses(BaseLandlordWorkflow, BaseLandlordData):

    """
    Fixes addresses that need to be corrected based on analysis results.

    INPUTS:
        - Validated addresses (master file)
        - Addresses to fix (3 different datasets)

    OUTPUTS:
        - Updated validated addresses master file
    """

    def __init__(self):
        super().__init__()
        self.df_valid_addrs_in = self.get_df(self.WKFL_VALIDATED_MASTER, LANDLORD_DTYPES)
        self.df_addrs_to_fix_in = self.get_df(self.TO_FIX, LANDLORD_DTYPES)
        self.df_addrs_to_fix_unit_in = self.get_df(self.TO_FIX_UNIT, LANDLORD_DTYPES)
        self.df_addrs_to_fix_missing_in = self.get_df(self.TO_FIX_MISSING, LANDLORD_DTYPES)
        self.df_outputs = {}

    def add_missing_units_to_formatted_address(self, df_addrs_to_fix_unit, df_addrs_to_fix_missing):
        df_unit_fixed = df_addrs_to_fix_unit.copy()
        df_missing_fixed = df_addrs_to_fix_missing.copy()
        df_unit_fixed["GCD_FORMATTED_ADDRESS"] = df_unit_fixed.apply(lambda row: self.fix_unit_num_formatted(row), axis=1)
        df_missing_fixed["GCD_FORMATTED_ADDRESS"] = df_missing_fixed.apply(lambda row: self.fix_unit_num_formatted(row), axis=1)
        return df_unit_fixed, df_missing_fixed

    def replace_fixed_addresses(self, df_addrs_to_fix, df_unit_fixed, df_missing_fixed):
        df_addrs_fixed = pd.concat([df_addrs_to_fix, df_unit_fixed, df_missing_fixed], ignore_index=True)
        df_valid_addrs_remove_old = self.df_valid_addrs_in[~self.df_valid_addrs_in["RAW_ADDRESS"].isin(df_addrs_fixed["RAW_ADDRESS"])]
        df_valid_addrs_updated_fixed = pd.concat([df_valid_addrs_remove_old, df_addrs_fixed], ignore_index=True)
        return df_valid_addrs_updated_fixed

    def finalize_validated_master(self, df_valid_addrs_updated_fixed):
        remove = ["1070 MAPLE WAY 13399, JACKSON, WY 83001", "1510 E 55TH ST 15072, CHICAGO, IL 60615"]
        df_valid_addrs_filtered = df_valid_addrs_updated_fixed[~df_valid_addrs_updated_fixed["GCD_FORMATTED_ADDRESS"].isin(remove)]
        df_valid_addrs_updated_final = df_valid_addrs_filtered.drop_duplicates(subset=["RAW_ADDRESS"])
        df_valid_addrs_updated_final["GCD_SECONDARYNUMBER"] = df_valid_addrs_updated_final["GCD_SECONDARYNUMBER"].apply(
            lambda x: x.replace("-", "") if pd.notnull(x) else x
        )
        return df_valid_addrs_updated_final

    def workflow(self):

        # LOAD DATA
        df_addrs_to_fix = self.df_addrs_to_fix_in
        df_addrs_to_fix_unit = self.df_addrs_to_fix_unit_in
        df_addrs_to_fix_missing = self.df_addrs_to_fix_missing_in

        # EXECUTE WORKFLOW
        df_unit_fixed, df_missing_fixed = self.add_missing_units_to_formatted_address(df_addrs_to_fix_unit, df_addrs_to_fix_missing)
        df_valid_addrs_updated_fixed = self.replace_fixed_addresses(df_addrs_to_fix, df_unit_fixed, df_missing_fixed)
        df_valid_addrs_updated_final = self.finalize_validated_master(df_valid_addrs_updated_fixed)

        # SET OUTPUTS
        self.df_outputs = {"validated_master": df_valid_addrs_updated_final}

    def save_output(self):
        self.save_df(self.df_outputs["validated_master"], self.WKFL_VALIDATED_MASTER, DATA_ROOT)


class WkflPrepareProps(BaseLandlordWorkflow, BaseLandlordData):

    """
    Prepares the property dataset by merging validated taxpayer addresses and adding property classes to facilitate
    rental property subsetting.

    INPUTS:
        - Scraped property data
        - Validated address master file
        - Datasets containing class codes for each property

    OUTPUTS:
        - Property dataset with validated addresses and a column containing building class codes
    """

    def __init__(self):
        super().__init__()
        self.df_scraped_in = self.get_df(self.COOK_SCRAPED_SUCCESS, LANDLORD_DTYPES)
        self.df_valid_addrs_in = self.get_df(self.WKFL_VALIDATED_MASTER, LANDLORD_DTYPES)
        self.df_less_6_in = self.get_df(self.PROPS_LESS_6, LANDLORD_DTYPES)
        self.df_greater_7_in = self.get_df(self.PROPS_GREATER_7, LANDLORD_DTYPES)
        self.df_pins_results_in = self.get_df(self.PINS_NO_CLASS_RESULTS, LANDLORD_DTYPES)
        self.df_outputs = {}

    def merge_validated_addresses(self):
        df_props_validated = pd.merge(self.df_scraped_in, self.df_valid_addrs_in, how="left", on="RAW_ADDRESS")
        df_props_validated = self.combine_columns_parallel(df_props_validated)
        df_props_validated = df_props_validated.drop_duplicates(subset=["PIN"])
        df_props_validated["GCD_FORMATTED_MATCH"] = df_props_validated.apply(lambda row: self.add_formatted_address_matching(row), axis=1)
        return df_props_validated

    def add_property_classes(self, df_greater_7, df_pins_results):
        df_greater_7_clean = df_greater_7.copy()
        df_pins_results_clean = df_pins_results.copy()
        df_greater_7_clean["KEYPIN"] = df_greater_7_clean["KEYPIN"].apply(lambda x: x.replace('-', '') if isinstance(x, str) else "")
        df_pins_results_clean["CLASS"] = df_pins_results_clean["CLASS"].apply(lambda x: self.fix_classes(x) if isinstance(x, str) else "")
        return df_greater_7_clean, df_pins_results_clean

    def merge_class_props(self, df_props_validated, df_less_6, df_greater_7_cleaned, df_pins_results_cleaned):
        df_merged1 = pd.merge(
            df_props_validated,
            df_less_6[["PIN", "CLASS"]],
            on="PIN",
            how="left"
        )
        df_merged2 = pd.merge(
            df_merged1,
            df_greater_7_cleaned[["KEYPIN", "CLASS(ES)"]],
            left_on="PIN",
            right_on="KEYPIN",
            how="left"
        )
        df_merged = pd.merge(
            df_merged2,
            df_pins_results_cleaned[["PIN", "CLASS"]],
            on="PIN",
            how="left"
        )
        return df_merged

    def clean_class_codes(self, df_merged):
        df_merged_clean = df_merged.copy()
        # remove dashes from class identifiers
        df_merged_clean["CLASS(ES)"] = df_merged_clean["CLASS(ES)"].apply(
            lambda x: x.replace('-', '') if isinstance(x, str) else ""
        )
        # combine class codes from disparate columns
        df_merged_clean["CLASS_CODE"] = df_merged_clean.apply(
            lambda row: self.combine_classes(row["CLASS_x"], row["CLASS(ES)"], row["CLASS_y"]),
            axis=1
        )
        return df_merged_clean

    def finalize_class_codes(self, df_merged_clean):
        df_merged_final = df_merged_clean.copy()
        # remove slashes from class code separators
        df_merged_final["CLASS_CODE"] = df_merged_final["CLASS_CODE"].apply(
            lambda x: x.replace('/', '') if isinstance(x, str) else ""
        )
        # final class code list clean up
        df_merged_final["CLASS_CODE"] = df_merged_final["CLASS_CODE"].apply(
            lambda x: self.fix_class_codes(x) if isinstance(x, str) else ""
        )
        df_merged_final = df_merged_final.drop(["PIN_2", "KEYPIN", "CLASS_x", "CLASS_y", "CLASS(ES)"], axis=1)
        df_merged_final = df_merged_final.drop_duplicates(subset=["PIN"])
        return df_merged_final


    def workflow(self):

        # LOAD DATA
        df_less_6 = self.df_less_6_in
        df_greater_7 = self.df_greater_7_in
        df_pins_results = self.df_pins_results_in

        # EXECUTE WORKFLOW
        df_props_validated = self.merge_validated_addresses()
        df_greater_7_clean, df_pins_results_clean = self.add_property_classes(df_greater_7, df_pins_results)
        df_merged = self.merge_class_props(df_props_validated, df_less_6, df_greater_7_clean, df_pins_results_clean)
        df_merged_clean = self.clean_class_codes(df_merged)
        df_merged_final = self.finalize_class_codes(df_merged_clean)

        # SET OUTPUTS
        self.df_outputs = {"props_validated_classes": df_merged_final}

    def save_outputs(self):
        self.save_df(self.df_outputs["props_validated_classes"], self.WKFL_PROPS_VALIDATED_CLASSES, DATA_ROOT)


class WkflSubsetRentals(BaseLandlordWorkflow, BaseLandlordData):

    """
    Subsets property data outputted by WkflPrepareProps for only rental properties.

    INPUTS:
        - Property dataset outputted from WkflPrepareProps

    OUTPUTS:
        - Rental-subsetted property dataset
    """

    def __init__(self):
        super().__init__()
        self.df_props_all_in = self.get_df(self.WKFL_PROPS_VALIDATED_CLASSES, LANDLORD_DTYPES)
        self.df_mto_in = self.get_df(self.PROPS_MTO, {"PIN": str})
        self.df_outputs = {}

    def subset_by_class_codes(self, df_props_all):
        df_props_is_rental = df_props_all.copy()
        df_props_is_rental["IS_RENTAL"] = df_props_is_rental["CLASS_CODE"].apply(
            lambda x: self.is_rental(x, self.RENTAL_CODES) if isinstance(x, str) else False
        )
        df_rentals = df_props_is_rental[df_props_is_rental["IS_RENTAL"] == True]
        return df_rentals

    def subset_by_taxpayer_address(self, df_rentals, df_props_all):
        """
        Fetch properties that were NOT pulled in from the class code subset by matching rental properties' taxpayer
        addresses with the non-rental properties.
        """
        rental_addrs = df_rentals["GCD_FORMATTED_ADDRESS"].unique().tolist()
        df_nonrentals = df_props_all[~df_props_all["PIN"].isin(rental_addrs)]
        df_rentals_missed = df_nonrentals[df_nonrentals["GCD_FORMATTED_ADDRESS"].isin(rental_addrs)]
        return df_rentals_missed

    def subset_by_mto_pins(self, df_mto, df_props_all):
        mto_pins = list(df_mto["PIN"].dropna().unique())
        df_props_mto = df_props_all[df_props_all["PIN"].isin(mto_pins)]
        return df_props_mto

    def finalize_rental_subset(self, df_rentals, df_rentals_missed, df_props_mto):
        df_rentals_final = pd.concat([df_rentals, df_rentals_missed, df_props_mto], ignore_index=True)
        df_rentals_final.drop_duplicates(subset=["PIN"], inplace=True)
        return df_rentals_final

    def workflow(self):

        # LOAD DATA
        df_props_all = self.df_props_all_in
        df_mto = self.df_mto_in

        # EXECUTE WORKFLOW
        df_rentals = self.subset_by_class_codes(df_props_all)
        df_rentals_missed = self.subset_by_taxpayer_address(df_rentals, df_props_all)
        df_props_mto = self.subset_by_mto_pins(df_mto, df_props_all)
        df_rentals_final = self.finalize_rental_subset(df_rentals, df_rentals_missed, df_props_mto)

        # SET OUTPUTS
        self.df_outputs = {"props_rentals": df_rentals_final}

    def save_outputs(self):
        self.save_df(self.df_outputs["props_rentals"], self.WKFL_PROPS_RENTALS, DATA_ROOT)


class WkflCleanTaxRecords(BaseLandlordWorkflow, CleanCorpLLC):

    """
    Cleans property taxpayer records. Adds boolean columns to be used at other stages in the workflow.

    INPUTS:
        - Rental-subsetted property dataset
        - Master names dataset
        - Common names dataset
        - Common addresses dataset
        - Landlord organization addresses dataset

    OUTPUTS:
        - Cleaned rental property dataset
    """

    def __init__(self):
        super().__init__()
        self.df_rentals_in = self.get_df(self.WKFL_PROPS_RENTALS, LANDLORD_DTYPES)
        self.df_names_in = self.get_df(self.MASTER_NAMES, LANDLORD_DTYPES)
        self.df_common_names_in = self.get_df(self.COMMON_NAMES, LANDLORD_DTYPES)
        self.df_common_addrs_in = self.get_df(self.COMMON_ADDRESSES, LANDLORD_DTYPES)
        self.df_org_addrs_in = self.get_df(self.LANDLORD_ORG_ADDRESSES, LANDLORD_DTYPES)
        self.df_outputs = {}
        self.names = list(self.df_names_in["NAME"])
        self.common_addrs = set(self.df_common_addrs_in["ADDRESS"].tolist())
        self.common_names = set(self.df_common_names_in["NAME"].tolist())
        self.org_addrs = set(self.df_org_addrs_in["ADDRESS"].tolist())

    def clean_taxpayer_names(self, df_rentals):
        """
        Adds the following fields:
            - CLEAN_NAME
            - CORE_NAME
        """
        df_rentals_cleaned = self.process_text_parallel_names(df_rentals)
        return df_rentals_cleaned

    def add_common_name_address_booleans(self, df_rentals_cleaned):
        """
        Adds the following fields:
            - IS_COMMON_NAME
            - IS_COMMON_ADDRESS
            - IS_LANDLORD_ORG_ADDRESS
        """
        df_rentals_bools = df_rentals_cleaned.copy()
        df_rentals_bools["IS_COMMON_NAME"] = df_rentals_bools["CLEAN_NAME"].apply(lambda x: x in self.common_names)
        df_rentals_bools["IS_COMMON_ADDRESS"] = df_rentals_bools["GCD_FORMATTED_ADDRESS"].apply(lambda x: x in self.common_addrs)
        df_rentals_bools["IS_LANDLORD_ORG_ADDRESS"] = df_rentals_bools["GCD_FORMATTED_ADDRESS"].apply(lambda x: x in self.org_addrs)
        return df_rentals_bools

    def add_corp_booleans(self, df_rentals_bools):
        """
        Adds the following fields:
            - CORP_WORDS
            - CORP_NUM
            - PEOPLE_STRUCTURE
            - CORP_SINGLE
            - CORP
        """
        df_corp_bools = self.ident_parallel(df_rentals_bools, self.names)
        return df_corp_bools

    def add_columns_banks_trusts(self, df_corp_bools):
        """
        Adds the following fields:
            - IS_BANK
            - IS_TRUST
            - TRUST_ID
        """
        df_cleaned_final = self.process_text_parallel_banks_trusts(df_corp_bools)
        return df_cleaned_final

    def workflow(self):

        # LOAD DATA
        df_rentals = self.df_rentals_in

        # EXECUTE WORKFLOW
        df_rentals_cleaned = self.clean_taxpayer_names(df_rentals)
        df_rentals_bools = self.add_common_name_address_booleans(df_rentals_cleaned)
        df_corp_bools = self.add_corp_booleans(df_rentals_bools)
        df_cleaned_final = self.add_columns_banks_trusts(df_corp_bools)

        # SET OUTPUTS
        self.df_outputs = {"props_rentals_cleaned": df_cleaned_final}

    def save_outputs(self):
        self.save_df(self.df_outputs["props_rentals_cleaned"], self.WKFL_PROPS_RENTALS_CLEANED, DATA_ROOT)


class WkflPrepareCorpLLC(BaseLandlordWorkflow, CleanCorpLLC, StringMatching):

    """
    Cleans corporate and LLC data and merges with validated rental subsetted properties. Filters to exclude inactive
    organizations; combines address columns to allow for easy validation; merges with property taxpayer names based on
    clean name, core name and string matches; subsets original corp/llc datasets to contain ONLY organizations matched
    with properties.

    INPUTS:
        - Corporate data master file
        - LLC data master file
        - Rental-subsetted property datasetn outputted by WkflCleanTaxRecords
        - Master names dataset
        - Common names dataset

    OUTPUTS:
        - Rental properties subset with corporate & LLC data merged
        - Corporate dataset subsetted for only corporations matched with properties
        - LLC dataset subsetted for only LLCs matched with properties
    """

    def __init__(self):
        super().__init__()
        self.df_corp_in = self.get_df(self.CORP_DATA, LANDLORD_DTYPES)
        self.df_llc_in = self.get_df(self.LLC_DATA, LANDLORD_DTYPES)
        self.df_props_in = self.get_df(self.WKFL_PROPS_RENTALS_CLEANED, LANDLORD_DTYPES)
        self.df_names_in = self.get_df(self.MASTER_NAMES, LANDLORD_DTYPES)
        self.df_common_names_in = self.get_df(self.COMMON_NAMES, LANDLORD_DTYPES)
        self.df_outputs = {}
        self.names = list(self.df_names_in["NAME"])
        self.common_names = set(self.df_common_names_in["NAME"].tolist())

    def subset_active_corps_llcs(self, df_corp, df_llc):
        df_corp_subset = df_corp[df_corp["CORP-STATUS"].isin([0,1])]
        df_llc_subset = df_llc[df_llc["LL-STATUS-CODE"].isin([0,1])]
        df_llc_subset = df_llc_subset.drop_duplicates(subset=["LL-FILE-NUMBER"])
        return df_corp_subset, df_llc_subset

    def set_corp_address_columns(self, df_corp_subset):
        df_corp_subset_addrs = df_corp_subset
        df_corp_subset_addrs["ADDRESS_1"] = df_corp_subset_addrs["CORP-PRES-NAME-ADDR_addr"]
        df_corp_subset_addrs["ADDRESS_2"] = df_corp_subset_addrs["CORP-SEC-NAME-ADDR_addr"]
        return df_corp_subset_addrs

    def fix_llc_zip_codes(self, df_llc_subset):
        df_llc_subset_fixed_zips = df_llc_subset
        df_llc_subset_fixed_zips["LL-RECORDS-OFF-ZIP-MAIN"] = df_llc_subset_fixed_zips["LL-RECORDS-OFF-ZIP"].apply(
            lambda x: self.set_zip_main(x)
        )
        df_llc_subset_fixed_zips["LL-RECORDS-OFF-ZIP-4"] = df_llc_subset_fixed_zips["LL-RECORDS-OFF-ZIP"].apply(
            lambda x: self.set_zip_4(x)
        )
        df_llc_subset_fixed_zips["LL-MM-ZIP-MAIN"] = df_llc_subset_fixed_zips["LL-MM-ZIP"].apply(
            lambda x: self.set_zip_main(x)
        )
        df_llc_subset_fixed_zips["LL-MM-ZIP-4"] = df_llc_subset_fixed_zips["LL-MM-ZIP"].apply(
            lambda x: self.set_zip_4(x)
        )
        df_llc_subset_fixed_zips["LL-AGENT-ZIP-MAIN"] = df_llc_subset_fixed_zips["LL-AGENT-ZIP"].apply(
            lambda x: self.set_zip_main(x)
        )
        df_llc_subset_fixed_zips["LL-AGENT-ZIP-4"] = df_llc_subset_fixed_zips["LL-AGENT-ZIP"].apply(
            lambda x: self.set_zip_4(x)
        )
        return df_llc_subset_fixed_zips

    def set_llc_address_columns(self, df_llc_subset_fixed_zips):
        df_llc_subset_addrs = df_llc_subset_fixed_zips
        df_llc_subset_addrs["ADDRESS_1"] = df_llc_subset_addrs.apply(
            lambda row: f"{row['LL-RECORDS-OFF-STREET']}, {row['LL-RECORDS-OFF-CITY']}, {row['LL-RECORDS-OFF-ZIP-MAIN']}",
            axis=1
        )
        df_llc_subset_addrs["ADDRESS_2"] = df_llc_subset_addrs.apply(
            lambda row: f"{row['LL-MM-STREET']}, {row['LL-MM-CITY']}, {row['LL-MM-ZIP-MAIN']}",
            axis=1
        )
        df_llc_subset_addrs["ADDRESS_3"] = df_llc_subset_addrs.apply(
            lambda row: f"{row['LL-AGENT-STREET']}, {row['LL-AGENT-ZIP-MAIN']}",
            axis=1
        )
        return df_llc_subset_addrs

    def extract_unique_corps_llcs(self, df_corp_subset_addrs, df_llc_subset_addrs):
        df_corp_unique = df_corp_subset_addrs[["CORP-NAME"]].drop_duplicates(subset=["CORP-NAME"])
        df_corp_unique.rename(columns={"CORP-NAME": "ENTITY_NAME"}, inplace=True)
        df_llc_unique = df_llc_subset_addrs[["LL-NAME"]].drop_duplicates(subset=["LL-NAME"])
        df_llc_unique.rename(columns={"LL-NAME": "ENTITY_NAME"}, inplace=True)
        df_corps_llcs = pd.concat([df_corp_unique, df_llc_unique], ignore_index=True)
        return df_corps_llcs

    def clean_corp_llc_names(self, df_corps_llcs):
        df_corps_llcs_cleaned = self.corp_data_parallel(df_corps_llcs)
        return df_corps_llcs_cleaned

    def merge_on_clean_name(self, df_props, df_corps_llcs_cleaned):
        df_merge = pd.merge(
            df_props, df_corps_llcs_cleaned, how="left", left_on="CLEAN_NAME", right_on="ENTITY_CLEAN_NAME"
        )
        df_merge_clean = self.combine_columns_parallel(df_merge)
        df_merge_clean["CLEAN_MERGE_ORG"] = df_merge_clean.apply(lambda row: self.set_clean_merge(row), axis=1)
        df_merge_clean = df_merge_clean.drop_duplicates(subset=["PIN"])
        return df_merge_clean

    def merge_on_core_name(self, df_merge_clean, df_corps_llcs_cleaned):
        df_core_unique = df_corps_llcs_cleaned.drop_duplicates(subset=["ENTITY_CORE_NAME"])
        df_merge_core = pd.merge(
            df_merge_clean, df_core_unique, how="left", left_on="CORE_NAME", right_on="ENTITY_CORE_NAME"
        )
        df_merge_core = self.combine_columns_parallel(df_merge_core)
        df_merge_core["CORE_MERGE_ORG"] = df_merge_core.apply(lambda row: self.set_core_merge(row), axis=1)
        df_merge_core = self.check_core_matches(df_merge_core, self.common_names)
        return df_merge_core

    def get_string_matches(self, df_merge_core, df_corps_llcs_cleaned):
        df_no_matches = df_merge_core[(df_merge_core["ENTITY_NAME"].isnull()) & (df_merge_core["CORP"] == True)]
        df_matches = self.match_strings(
            ref_docs=list(df_corps_llcs_cleaned["ENTITY_CLEAN_NAME"].dropna().unique()),
            query_docs=list(df_no_matches["CLEAN_NAME"].dropna().unique()),
            nmslib_opts={
                "method": "hnsw",
                "space": "cosinesimil_sparse_fast",
                "data_type": nmslib.DataType.SPARSE_VECTOR
            },
            query_batch_opts={
                "num_threads": 8,
                "K": 1
            },
            match_threshold=.89
        )
        return df_matches

    def merge_on_string_matches(self, df_merge_core, df_matches):
        df_merge_string_match = pd.merge(
            df_merge_core, df_matches, how="left", left_on="ENTITY_CLEAN_NAME", right_on="ORIGINAL_DOC"
        )
        df_merge_string_match["FUZZY_MERGE_ORG"] = df_merge_core.apply(
            lambda row: self.set_fuzzy_merge(row), axis=1
        )
        df_merge_string_match.drop(columns=["ORIGINAL_DOC", "MATCHED_DOC", "CONF", "LDIST", "CONF1"], inplace=True)
        return df_merge_string_match

    def subset_corps_llcs(self, df_merge_string_match, df_corp_subset_addrs, df_llc_subset_addrs):
        entity_names = list(df_merge_string_match["ENTITY_NAME"].dropna().unique())
        df_corps_props = df_corp_subset_addrs[df_corp_subset_addrs["CORP-NAME"].isin(entity_names)]
        df_llc_props = df_llc_subset_addrs[df_llc_subset_addrs["LL-NAME"].isin(entity_names)]
        return df_corps_props, df_llc_props

    def workflow(self):

        # LOAD DATA
        df_corp = self.df_corp_in
        df_llc = self.df_llc_in
        df_props = self.df_props_in

        # EXECUTE WORKFLOW
        df_corp_subset, df_llc_subset = self.subset_active_corps_llcs(df_corp, df_llc)
        df_corp_subset_addrs = self.set_corp_address_columns(df_corp_subset)
        df_llc_subset_fixed_zips = self.fix_llc_zip_codes(df_llc_subset)
        df_llc_subset_addrs = self.set_llc_address_columns(df_llc_subset_fixed_zips)
        df_corps_llcs = self.extract_unique_corps_llcs(df_corp_subset_addrs, df_llc_subset_addrs)
        df_corps_llcs_cleaned = self.clean_corp_llc_names(df_corps_llcs)
        df_merge_clean = self.merge_on_clean_name(df_props, df_corps_llcs_cleaned)
        df_merge_core = self.merge_on_core_name(df_merge_clean, df_corps_llcs_cleaned)
        df_matches = self.get_string_matches(df_merge_core, df_corps_llcs_cleaned)
        df_merge_string_match = self.merge_on_string_matches(df_merge_core, df_matches)
        df_corps_props, df_llcs_props = self.subset_corps_llcs(
            df_merge_string_match, df_corp_subset_addrs, df_llc_subset_addrs
        )

        # SET OUTPUTS
        self.df_outputs = {
            "corp_props": df_corps_props,
            "llc_props": df_llcs_props,
            "props_entities": df_merge_string_match
        }

    def save_outputs(self):
        self.save_df(self.df_outputs["corp_props"], self.WKFL_CORP_PROPS, DATA_ROOT)
        self.save_df(self.df_outputs["llc_props"], self.WKFL_LLC_PROPS, DATA_ROOT)
        self.save_df(self.df_outputs["props_entities"], self.WKFL_PROPS_ENTITIES, DATA_ROOT)


class WkflValidateCorpLLC(BaseLandlordWorkflow, CleanCorpLLC):

    """
    Merges validated address data to corporate and LLC subsets outputted from WkflPrepareCorpLLC. Executes one merge
    per address column (three merges for LLC data, two for corporate data). Once these merges are complete, the
    resulting validated corporate and LLC datasets are merged with the rental properties dataset. Finally, address
    matching columns are created for each corporate/llc address column (remove secondary unit signifier to increase
    matches with unit/suite number).

    INPUTS:
        - Master validated address file
        - LLC dataset subsetted for properties
        - Corporate dataset subsetted for properties
        - Rental-subsetted property dataset outputted by WkflPrepareCorpLLC

    OUTPUTS:
        - Corporate dataset subsetted for rental properties with validated address data
        - LLC dataset subsetted for rental properties with validated address data
        - Rental properties dataset containing validated address data for associated corporations/LLCs
    """

    def __init__(self):

        super().__init__()

        self.df_valid_addrs_in = self.get_df(self.WKFL_VALIDATED_MASTER, LANDLORD_DTYPES)
        self.df_corp_props_in = self.get_df(self.WKFL_CORP_PROPS, LANDLORD_DTYPES)
        self.df_llc_props_in = self.get_df(self.WKFL_LLC_PROPS, LANDLORD_DTYPES)
        self.df_rentals_in = self.get_df(self.WKFL_PROPS_ENTITIES, LANDLORD_DTYPES)
        self.df_common_addrs_in = self.get_df(self.COMMON_ADDRESSES, LANDLORD_DTYPES)
        self.df_org_addrs_in = self.get_df(self.LANDLORD_ORG_ADDRESSES, LANDLORD_DTYPES)
        self.df_outputs = {}
        self.common_addrs = set(self.df_common_addrs_in["ADDRESS"].tolist())
        self.org_addrs = set(self.df_org_addrs_in["ADDRESS"].tolist())

    def drop_dups_add_is_corp_llc_booleans(self, df_corp_props, df_llc_props):
        df_llc_props_cleaned = df_llc_props.drop_duplicates(subset=["LL-NAME"])
        df_llc_props_cleaned["IS_LLC"] = True
        df_corp_props_cleaned = df_corp_props.drop_duplicates(subset=["CORP-NAME"])
        df_corp_props_cleaned["IS_CORP"] = True
        return df_corp_props_cleaned, df_llc_props_cleaned

    def merge_corps_on_addresses(self, df_corp_props_cleaned, df_valid_addrs):
        df_corp_merged1 = self.merge_gcd_orgs(
            df_corp_props_cleaned, df_valid_addrs, left_on="ADDRESS_1", dup_col_drop="CORP-FILE-NUMBER"
        )
        df_corp_merged = self.merge_gcd_orgs(
            df_corp_merged1, df_valid_addrs, left_on="ADDRESS_2", dup_col_drop="CORP-FILE-NUMBER"
        )
        return df_corp_merged

    def merge_llcs_on_addresses(self, df_llc_props_cleaned, df_valid_addrs):
        df_llc_merged1 = self.merge_gcd_orgs(
            df_llc_props_cleaned, df_valid_addrs, left_on="ADDRESS_1", dup_col_drop="LL-FILE-NUMBER"
        )
        df_llc_merged2 = self.merge_gcd_orgs(
            df_llc_merged1, df_valid_addrs, left_on="ADDRESS_2", dup_col_drop="LL-FILE-NUMBER"
        )
        df_llc_merged = self.merge_gcd_orgs(
            df_llc_merged2, df_valid_addrs, left_on="ADDRESS_3", dup_col_drop="LL-FILE-NUMBER"
        )
        return df_llc_merged

    def clean_address_merges(self, df_corp_merged, df_llc_merged):
        df_corp_cleaned = df_corp_merged.copy()
        df_llc_cleaned = df_llc_merged.copy()
        df_corp_cleaned["CORP_NAME_CLEAN"] = df_corp_cleaned["CORP-NAME"].apply(lambda x: self.clean_names_corp(x))
        df_llc_cleaned["LL_NAME_CLEAN"] = df_llc_cleaned["LL-NAME"].apply(lambda x: self.clean_names_corp(x))
        df_corp_merged_final = self.combine_columns_parallel(df_corp_cleaned)
        df_llc_merged_final = self.combine_columns_parallel(df_llc_cleaned)
        return df_corp_merged_final, df_llc_merged_final

    def merge_props_on_corps(self, df_corp_merged_final, df_llc_merged_final, df_rentals):
        df_rentals_merged_llc = pd.merge(df_rentals, df_corp_merged_final, how="left", left_on="ENTITY_NAME", right_on="LL-NAME")
        df_rentals_merged_corp = pd.merge(df_rentals_merged_llc, df_llc_merged_final, how="left", left_on="ENTITY_NAME", right_on="CORP-NAME")
        df_rentals_merged = self.combine_columns_parallel(df_rentals_merged_corp)
        return df_rentals_merged

    def clean_merge_columns(self, df_rentals_merged):
        df_rentals_merged_cleaned = df_rentals_merged.drop(columns=[
            "LL-STATUS-CODE",
            "LL-RECORDS-OFF-STREET",
            "LL-RECORDS-OFF-CITY",
            "LL-RECORDS-OFF-ZIP",
            "LL-AGENT-CODE",
            "LL-AGENT-STREET",
            "LL-AGENT-ZIP",
            "LL-AGENT-COUNTY-CODE",
            "LL-MM-STREET",
            "LL-MM-CITY",
            "LL-MM-ZIP",
            "LL-RECORDS-OFF-ZIP-MAIN",
            "LL-RECORDS-OFF-ZIP-4",
            "LL-MM-ZIP-MAIN",
            "LL-MM-ZIP-4",
            "LL-AGENT-ZIP-MAIN",
            "LL-AGENT-ZIP-4",
            "CORP-STATUS",
            "CORP-PRES-NAME-ADDR",
            "CORP-SEC-NAME-ADDR",
            "CORP-PRES-NAME-ADDR_addr",
            "CORP-PRES-NAME-ADDR_extract_status",
            "CORP-SEC-NAME-ADDR_addr",
            "CORP-SEC-NAME-ADDR_extract_status",
        ])
        if "Unnamed: 0" in df_rentals_merged_cleaned.columns:
            df_rentals_merged_cleaned.drop(columns=["Unnamed: 0"], inplace=True)
        return df_rentals_merged_cleaned

    def set_address_matching_columns(self, df_rentals_merged_cleaned):
        df_rentals_match_addresses = df_rentals_merged_cleaned
        df_rentals_match_addresses["GCD_FORMATTED_ADDRESS_ADDRESS_1_MATCH"] = df_rentals_match_addresses.apply(
            lambda row: self.add_formatted_address_matching(row, "_ADDRESS_1"), axis=1
        )
        df_rentals_match_addresses["GCD_FORMATTED_ADDRESS_ADDRESS_2_MATCH"] = df_rentals_match_addresses.apply(
            lambda row: self.add_formatted_address_matching(row, "_ADDRESS_2"), axis=1
        )
        df_rentals_match_addresses["GCD_FORMATTED_ADDRESS_ADDRESS_3_MATCH"] = df_rentals_match_addresses.apply(
            lambda row: self.add_formatted_address_matching(row, "_ADDRESS_3"), axis=1
        )
        return df_rentals_match_addresses

    def set_address_booleans(self, df_rentals_match_addresses):
        df_rentals_final = df_rentals_match_addresses.copy()
        df_rentals_final["IS_COMMON_ADDRESS_1"] = df_rentals_final["GCD_FORMATTED_ADDRESS_ADDRESS_1"].apply(lambda x: x in self.common_addrs if pd.notnull(x) else np.nan)
        df_rentals_final["IS_COMMON_ADDRESS_2"] = df_rentals_final["GCD_FORMATTED_ADDRESS_ADDRESS_2"].apply(lambda x: x in self.common_addrs if pd.notnull(x) else np.nan)
        df_rentals_final["IS_COMMON_ADDRESS_3"] = df_rentals_final["GCD_FORMATTED_ADDRESS_ADDRESS_3"].apply(lambda x: x in self.common_addrs if pd.notnull(x) else np.nan)
        df_rentals_final["IS_LANDLORD_ORG_ADDRESS_1"] = df_rentals_final["GCD_FORMATTED_ADDRESS_ADDRESS_1"].apply(lambda x: x in self.org_addrs if pd.notnull(x) else np.nan)
        df_rentals_final["IS_LANDLORD_ORG_ADDRESS_2"] = df_rentals_final["GCD_FORMATTED_ADDRESS_ADDRESS_2"].apply(lambda x: x in self.org_addrs if pd.notnull(x) else np.nan)
        df_rentals_final["IS_LANDLORD_ORG_ADDRESS_3"] = df_rentals_final["GCD_FORMATTED_ADDRESS_ADDRESS_3"].apply(lambda x: x in self.org_addrs if pd.notnull(x) else np.nan)
        return df_rentals_final

    def workflow(self):

        # LOAD DATA
        df_llc_props = self.df_llc_props_in
        df_corp_props = self.df_corp_props_in
        df_valid_addrs = self.df_valid_addrs_in
        df_rentals = self.df_rentals_in

        # EXECUTE WORKFLOW
        df_corp_props_cleaned, df_llc_props_cleaned = self.drop_dups_add_is_corp_llc_booleans(df_corp_props, df_llc_props)
        df_corp_merged = self.merge_corps_on_addresses(df_corp_props_cleaned, df_valid_addrs)
        df_llc_merged = self.merge_llcs_on_addresses(df_llc_props_cleaned, df_valid_addrs)
        df_corp_merged_final, df_llc_merged_final = self.clean_address_merges(df_corp_merged, df_llc_merged)
        df_rentals_merged = self.merge_props_on_corps(df_llc_merged_final, df_corp_merged_final, df_rentals)
        df_rentals_merged_cleaned = self.clean_merge_columns(df_rentals_merged)
        df_rentals_match_addresses = self.set_address_matching_columns(df_rentals_merged_cleaned)
        df_rentals_final = self.set_address_booleans(df_rentals_match_addresses)

        # SET OUTPUTS
        self.df_outputs = {
            "llc_props_validated": df_llc_merged_final,
            "corp_props_validated": df_corp_merged_final,
            "props_entities_validated": df_rentals_final
        }

    def save_outputs(self):
        self.save_df(self.df_outputs["llc_props_validated"], self.WKFL_LLC_PROPS_VALIDATED, DATA_ROOT)
        self.save_df(self.df_outputs["corp_props_validated"], self.WKFL_CORP_PROPS_VALIDATED, DATA_ROOT)
        self.save_df(self.df_outputs["props_entities_validated"], self.WKFL_PROPS_ENTITIES_VALIDATED, DATA_ROOT)


class WkflTaxStringMatch(BaseLandlordWorkflow, StringMatching, NetworkAnalysis):

    """
    Runs string matching on taxpayer name + address concatenation. Runs string matching results through network
    analysis to associate properties to their matches. Merges network analysis results to the rental property dataset.

    INPUTS:
        - Rental property dataset outputted by WkflMergeValidatedCorpsLlcs
        - Master validated address file
        - Common addresses dataset
        - Landlord organization addresses dataset

    OUTPUTS:
        - Rental property dataset with merged string matching results
    """

    def __init__(self):
        super().__init__()
        self.df_rentals_in = self.get_df(self.WKFL_PROPS_ENTITIES_VALIDATED, LANDLORD_DTYPES)
        self.df_valid_addrs_in = self.get_df(self.WKFL_VALIDATED_MASTER, LANDLORD_DTYPES)
        self.df_common_addrs_in = self.get_df(self.COMMON_ADDRESSES, LANDLORD_DTYPES)
        self.df_org_addrs_in = self.get_df(self.LANDLORD_ORG_ADDRESSES, LANDLORD_DTYPES)
        self.df_analysis_in  = self.get_df(self.ADDRESS_ANALYSIS_MASTER, LANDLORD_DTYPES)
        self.df_outputs = {}
        self.addrs_to_exclude = list(set(list(self.df_common_addrs_in["ADDRESS"]) + list(self.df_org_addrs_in["ADDRESS"])))

    def run_string_matching(self, df_rentals: pd.DataFrame, df_analysis: pd.DataFrame):

        # create summary stats dataframe
        df_summary_stats: pd.DataFrame = pd.DataFrame(
            columns=[
                "taxpayer_col",
                "include_orgs?",
                "include_unresearched?",
                "match_threshold",
                "match_name",
                "unique_tax_records_clean",
                "unique_tax_records_core",
                "matched_properties",
                "unique_matches",
                "top_100_bldg_count"
            ]
        )

        name_address_column = {
            "CLEAN_NAME": "NAME_ADDRESS_CLEAN",
            "CORE_NAME": "NAME_ADDRESS_CORE"
        }

        params_matrix = [
            {
                "taxpayer_name_col": "CLEAN_NAME",
                "include_orgs": False,
                "include_unresearched": False,
                "match_threshold": 0.85,
            },
            {
                "taxpayer_name_col": "CLEAN_NAME",
                "include_orgs": False,
                "include_unresearched": True,
                "match_threshold": 0.85,
            },
            {
                "taxpayer_name_col": "CLEAN_NAME",
                "include_orgs": False,
                "include_unresearched": True,
                "match_threshold": 0.8,
            },
        ]

        # fetch copy of df_rentals for merging
        df_rentals_prematch = df_rentals
        df_rentals_prematch["CLEAN_ADDRESS"] = df_rentals_prematch.apply(lambda row: self.set_cleaned_address(row), axis=1)
        df_rentals_prematch["NAME_ADDRESS_CLEAN"] = df_rentals_prematch["CLEAN_NAME"] + ' - ' + df_rentals_prematch["CLEAN_ADDRESS"]
        df_rentals_prematch["NAME_ADDRESS_CORE"] = df_rentals_prematch["CORE_NAME"] + ' - ' + df_rentals_prematch["CLEAN_ADDRESS"]
        unique_tax_records_clean = len(df_rentals_prematch["NAME_ADDRESS_CLEAN"].dropna().unique())
        unique_tax_records_core = len(df_rentals_prematch["NAME_ADDRESS_CORE"].dropna().unique())

        # loop through unique combinations of param matrix options
        for i, params in enumerate(params_matrix):

            print("taxpayer_col:", params["taxpayer_name_col"])
            print("include_orgs:", params["include_orgs"])
            print("include_unresearched:", params["include_unresearched"])
            print("match_threshold:", params["match_threshold"])

            df_filtered_analyzed = df_rentals_prematch

            print("Setting INCLUDE_ADDRESS column...")
            df_filtered_analyzed["INCLUDE_ADDRESS"] = df_filtered_analyzed["CLEAN_ADDRESS"].apply(
                lambda x: self.check_address(x, df_analysis, params["include_orgs"], params["include_unresearched"])
            )

            print("Filtering INCLUDE_ADDRESS column...")
            df_filtered = df_filtered_analyzed[df_filtered_analyzed["INCLUDE_ADDRESS"] == True]
            df_filtered = df_filtered[[
                "CLEAN_NAME", "CORE_NAME", "CLEAN_ADDRESS", "NAME_ADDRESS_CLEAN", "NAME_ADDRESS_CORE"
            ]]

            print("Commencing string matching...")
            df_matches = self.match_strings(
                ref_docs=list(df_filtered[name_address_column[params["taxpayer_name_col"]]].dropna().unique()),
                query_docs=list(df_filtered[name_address_column[params["taxpayer_name_col"]]].dropna().unique()),
                nmslib_opts= {
                    "method": "hnsw",
                    "space": "cosinesimil_sparse_fast",
                    "data_type": nmslib.DataType.SPARSE_VECTOR
                },
                query_batch_opts={
                    "num_threads": 8,
                    "K": 3
                },
                match_threshold=params["match_threshold"]
            )

            print("String matching complete. Creating network graph...")
            gMatches = self.name_addr_network(df_matches)
            df_matches_networked = self.process_name_addr_network(
                i, df_filtered, df_matches, gMatches, name_address_column[params["taxpayer_name_col"]]
            )

            print("Network graph complete. Merging...")
            df_rentals_prematch = pd.merge(df_rentals_prematch, df_matches_networked, how="left", on=name_address_column[params["taxpayer_name_col"]])
            df_rentals_prematch = self.combine_columns_parallel(df_rentals_prematch)
            df_rentals_prematch = df_rentals_prematch.drop_duplicates(subset=["PIN"])
            df_rentals_prematch.drop(columns=["ORIGINAL_DOC"], inplace=True)

            # fetch summary statistics
            matched_properties: int = len(df_rentals_prematch[f"STRING_MATCHED_NAME_{i+1}"].dropna())
            unique_matches: int = len(df_rentals_prematch[f"STRING_MATCHED_NAME_{i+1}"].dropna().unique())
            top_100: List[Tuple[str, int]] = Counter(
                list(df_rentals_prematch[f"STRING_MATCHED_NAME_{i+1}"].dropna())
            ).most_common(100)
            top_100_bldg_count: int = len(
                df_rentals_prematch[df_rentals_prematch[f"STRING_MATCHED_NAME_{i+1}"].isin(
                    [name for name, count in top_100]
                )]
            )

            # add summary stats
            df_summary_stats.loc[len(df_summary_stats)] = [
                params["taxpayer_name_col"],
                params["include_orgs"],
                params["include_unresearched"],
                params["match_threshold"],
                f"STRING_MATCHED_NAME_{i+1}",
                unique_tax_records_clean,
                unique_tax_records_core,
                matched_properties,
                unique_matches,
                top_100_bldg_count
            ]

        return {
            "string_match_results": df_rentals_prematch,
            "summary_stats": df_summary_stats
        }

    def workflow(self):

        # LOAD DATA
        df_rentals = self.df_rentals_in
        df_analysis = self.df_analysis_in

        # EXECUTE WORKFLOW
        string_match_results: Dict[str, pd.DataFrame] = self.run_string_matching(df_rentals, df_analysis)

        # SET OUTPUTS
        self.df_outputs = {
            "props_string_matched": string_match_results["string_match_results"],
            "summary_stats": string_match_results["summary_stats"]
        }

    def save_outputs(self):
        self.save_df(self.df_outputs["props_string_matched"], self.WKFL_PROPS_STRING_MATCHED_TEST, DATA_ROOT)
        self.save_df(self.df_outputs["summary_stats"], self.STRING_MATCH_SUMMARY_STATS, DATA_ROOT)


class WkflNetworkAnalysis(BaseLandlordWorkflow, NetworkAnalysis):

    """
    Runs property dataset through final network analysis function.

    INPUTS:
        - Rental property dataset outputted by WkflMergeValidatedCorpsLlcs
        - Master validated address file
        - Common names dataset
        - Common addresses dataset
        - Landlord organization addresses dataset

    OUTPUTS:
        - Rental property dataset with landlord networks merged
    """

    def __init__(self):
        super().__init__()
        self.df_rentals_in = self.get_df(self.WKFL_PROPS_STRING_MATCHED_TEST, LANDLORD_DTYPES)
        self.df_common_names_in = self.get_df(self.COMMON_NAMES, LANDLORD_DTYPES)
        self.df_common_addrs_in = self.get_df(self.COMMON_ADDRESSES, LANDLORD_DTYPES)
        self.df_org_addrs_in = self.get_df(self.LANDLORD_ORG_ADDRESSES, LANDLORD_DTYPES)
        self.df_analysis_in  = self.get_df(self.ADDRESS_ANALYSIS_MASTER, LANDLORD_DTYPES)
        self.df_outputs = {}
        self.common_names = list(self.df_common_names_in["NAME"].unique())
        self.common_addrs = list(self.df_common_addrs_in["ADDRESS"].unique())
        self.org_addrs = list(self.df_org_addrs_in["ADDRESS"].unique())
        self.common_org_combined = list(set(self.common_addrs + self.org_addrs))


    def run_network_analysis(self, df_rentals: pd.DataFrame, df_analysis: pd.DataFrame):

        # create summary stats dataframe
        df_summary_stats: pd.DataFrame = pd.DataFrame(
            columns=[
                "taxpayer_col",
                "entity_col",
                "include_orgs?",
                "include_unresearched?",
                "string_match_name",
                "network_name",
                "bldg_count_no_ntwk",
                "unique_ntwk_count",
                "top_100_bldg_count"
            ]
        )

        params_matrix = [
            {
                "taxpayer_name_col": "CLEAN_NAME",
                "entity_name_col": "ENTITY_CLEAN_NAME",
                "org_options": False,
                "unresearched_options": False,
                "string_match_names": "STRING_MATCHED_NAME_1"
            },
            {
                "taxpayer_name_col": "CLEAN_NAME",
                "entity_name_col": "ENTITY_CORE_NAME",
                "org_options": False,
                "unresearched_options": False,
                "string_match_names": "STRING_MATCHED_NAME_3"
            },
            {
                "taxpayer_name_col": "CORE_NAME",
                "entity_name_col": "ENTITY_CLEAN_NAME",
                "org_options": False,
                "unresearched_options": True,
                "string_match_names": "STRING_MATCHED_NAME_3"
            },
            {
                "taxpayer_name_col": "CLEAN_NAME",
                "entity_name_col": "ENTITY_CLEAN_NAME",
                "org_options": True,
                "unresearched_options": False,
                "string_match_names": "STRING_MATCHED_NAME_2"
            },
            {
                "taxpayer_name_col": "CLEAN_NAME",
                "entity_name_col": "ENTITY_CLEAN_NAME",
                "org_options": True,
                "unresearched_options": True,
                "string_match_names": "STRING_MATCHED_NAME_3"
            },
            {
                "taxpayer_name_col": "CORE_NAME",
                "entity_name_col": "ENTITY_CLEAN_NAME",
                "org_options": True,
                "unresearched_options": True,
                "string_match_names": "STRING_MATCHED_NAME_3"
            }
        ]

        # set options for params matrix
        # taxpayer_name_cols = ["CLEAN_NAME", "CORE_NAME"]
        # entity_name_cols = ["ENTITY_CLEAN_NAME", "ENTITY_CORE_NAME"]
        # org_options = [False, True]
        # unresearched_options = [False, True]
        # string_match_names = ["STRING_MATCHED_NAME_1", "STRING_MATCHED_NAME_5", "STRING_MATCHED_NAME_3", "STRING_MATCHED_NAME_4"]
        # taxpayer_name_cols = ["CLEAN_NAME"]
        # entity_name_cols = ["ENTITY_CLEAN_NAME"]
        # org_options = [False]
        # unresearched_options = [False]
        # string_match_names = ["STRING_MATCHED_NAME_1"]

        # fetch copy of df_rentals for merging
        df_networked = df_rentals.copy()

        # loop through unique combinations of param matrix options
        # for i, params in enumerate(product(
        #         taxpayer_name_cols, entity_name_cols, org_options, unresearched_options, string_match_names
        # )):

        for i, params in enumerate(params_matrix):

            print("taxpayer_col:", params["taxpayer_name_col"])
            print("entity_col:", params["entity_name_col"])
            print("include_orgs:", params["org_options"])
            print("include_unresearched:", params["unresearched_options"])
            print("string_matched_name:", params["string_match_names"])

            # run network analysis
            df_rentals_component, gMatches = self.rentals_network(
                network_id=i,
                df_rentals=df_rentals,
                df_analysis=df_analysis,
                string_match_column=params["string_match_names"],
                clean_core_column=params["taxpayer_name_col"],
                clean_core_column_entity=params["entity_name_col"],
                include_orgs=params["org_options"],
                include_unresearched=params["unresearched_options"],
            )
            # merge network analysis results into df_rentals copy
            df_networked = pd.merge(
                df_networked,
                df_rentals_component[["NAME_ADDRESS_CLEAN", f"FINAL_COMPONENT_{i+1}"]],
                how="left",
                on="NAME_ADDRESS_CLEAN"
            )
            # calculate network name column
            df_networked = self.set_network_name(
                i+1,
                df_networked,
                f"FINAL_COMPONENT_{i+1}",
                f"CHI_NETWORK_{i+1}"
            )

            df_networked = self.set_network_text(
                gMatches,
                df_networked,
                f"CHI_NETWORK_{i+1}",
                f"FINAL_COMPONENT_{i+1}"
            )

            # summary stats calculations
            bldg_count_no_ntwk: int = len(df_networked[df_networked[f"CHI_NETWORK_{i+1}"].isnull()])
            unique_ntwk_count: int = len(df_networked[f"CHI_NETWORK_{i+1}"].dropna().unique())
            top_100: List[Tuple[str, int]] = Counter(list(df_networked[f"CHI_NETWORK_{i+1}"])).most_common(100)
            top_100_bldg_count: int = len(df_networked[df_networked[f"CHI_NETWORK_{i+1}"].isin(
                [component for component, count in top_100]
            )])

            # add summary stats
            df_summary_stats.loc[len(df_summary_stats)] = [
                params["taxpayer_name_col"],
                params["entity_name_col"],
                params["org_options"],
                params["unresearched_options"],
                params["string_match_names"],
                f"CHI_NETWORK_{i+1}",
                bldg_count_no_ntwk,
                unique_ntwk_count,
                top_100_bldg_count,
            ]

        return {
            "network_results": df_networked,
            "summary_stats": df_summary_stats,
            "gMatches": gMatches,
        }

    def workflow(self):

        # LOAD DATA
        df_rentals = self.df_rentals_in
        df_analysis = self.df_analysis_in

        # EXECUTE WORKFLOW
        network_results = self.run_network_analysis(df_rentals, df_analysis)

        # TO-DO:
        # 4. run several times with different string matching threshold & analyze results
        # 5. run with and without including string-matched fuzzy_name_combo values in the main network graph builder (#2 in main for loop in rentals_network())

        # SET OUTPUTS
        self.df_outputs = {
            "props_networked": network_results["network_results"],
            "summary_stats": network_results["summary_stats"]
        }

    def save_outputs(self):
        self.save_df(self.df_outputs["props_networked"], self.WKFL_PROPS_NETWORKED_TEST, DATA_ROOT)
        self.save_df(self.df_outputs["summary_stats"], self.NETWORK_SUMMARY_STATS, DATA_ROOT)
